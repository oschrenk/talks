<!doctype html>
<html lang="en">

	<head>
		<meta charset="utf-8">

		<title>Hadoop - Riding the Elephant</title>

		<meta name="description" content="Hadoop - Riding the Elephant">
		<meta name="author" content="Oliver Schrenk">

		<meta name="apple-mobile-web-app-capable" content="yes" />
		<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />

		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<link rel="stylesheet" href="css/reveal.min.css">
		<link rel="stylesheet" href="css/theme/night.css	" id="theme">

		<!-- For syntax highlighting -->
		<link rel="stylesheet" href="lib/css/zenburn.css">

		<!-- If the query includes 'print-pdf', use the PDF print sheet -->
		<script>
			document.write( '<link rel="stylesheet" href="css/print/' + ( window.location.search.match( /print-pdf/gi ) ? 'pdf' : 'paper' ) + '.css" type="text/css" media="print">' );
		</script>

		<!--[if lt IE 9]>
		<script src="lib/js/html5shiv.js"></script>
		<![endif]-->
	</head>

	<body>

		<div class="reveal">

			<!-- Any section element inside of this container is displayed as a slide -->
			<div class="slides">

				<section>
					<h1>Hadoop</h1>
					<h3>Riding the Elephant</h3>
					<p>
						<small>Created by <a href="http://oschrenk.com">Oliver Schrenk</a> / <a href="http://twitter.com/oschrenk">@oschrenk</a></small>
					</p>
				</section>

				<section>
					<h1>Motivation</h1>
					<ul>
						<li>traditionally computation has been processor bound</li>
						<li>relatively small of amounts of data</li>
						<li>complex processing performed on that data</li>
					</ul>
				</section>

				<section>
					<h1>Motivation</h1>
					<ul>
						<li>nowadays we have much more data</li>
						<li>problem is getting the data to the processor</li>
					</ul>
				</section>

				<section>
					<h1>Motivation</h1>
					<h3>Transferring 100 GB</h3>
					<ul>
						<li>Consumer HDD (7k) 150 MB/s  <b>10.5 m</b></li>
						<li>Enterprise HDD (15k) 225 MB/s <b>7.0 m</b></li>
						<li>SSD 450+ MB/s <b>3.5 m</b></li>
					</ul>
				</section>

				<section>
					<h1>Motivation</h1>
					<h3>Transferring 100 GB</h3>
					<ul>
						<li>1Gb Network Theoretical Limit 125 MB/s  <b>12.7 m</b></li>
						<li>Cyso Downstream 2.4 MB/s <b>662.3 m</b></li>
					</ul>
				</section>

				<section>
					<h1>Motivation</h1>
					<ul>
						<li>hardware can fail</li>
						<li>data may be corrupt</li>
					</ul>
				</section>

				<section>
					<h1>Motivation</h1>
					<ul>
						<li>traditional distributed systems are hard<br>(temporal dependencies)</li>
					</ul>
				</section>

				<section>
					<h1>Concepts</h1>
					<ul>
						<li>bringing the computation to the data</li>
						<li>fault tolerance</li>
						<li>high level abstractions</li>
					</ul>
				</section>

				<section>
					<h1>Hadoop</h1>
					<ul>
						<li>Large scale, open source software framework</li>
						<li>Dedicated to scalable, distributed, data-intensive computing</li>
						<li>Handles thousands of nodes and petabytes of data</li>
					</ul>
				</section>

				<section>
					<h1>Hadoop</h1>
					<h3>two components</h3>
				</section>

				<section>
					<h1>HDFS</h1>
					<h3>Hadoop Distributed File System</h3>
				</section>

				<section>
					<h1>HDFS</h1>
					<h3>Hadoop Distributed File System</h3>
					<ul>
						<li>based on GFS</li>
						<li>on top of native file system</li>
						<li>user-space Java process</li>
						<li>optimized for throughput</li>
						<li>write once, streaming reads</li>
						<li>unpadded blocks of 128 MB</li>
					</ul>
				</section>

				<section>
					<h1>HDFS</h1>
					<h3>Hadoop Distributed File System</h3>
					<ul>
						<li>follows Master Slave System</li>
						<li>Master: NameNode, keeps metadata</li>
						<li>Slave: DataNode, keeps data</li>
					</ul>
				</section>

				<section>
					<h1>HDFS</h1>
					<h3>Hadoop Distributed File System</h3>
					<pre><code data-trim contenteditable>
# copy local file to user's home on HDFS
hadoop fs -put foo.txt foo.txt
# list files in user's home directory
hadoop fs -ls
# display content of a file
hadoop fs –cat /user/fred/bar.txt
# move file to local machine
hadoop fs –get /user/fred/bar.txt baz.txt
# Create directory
hadoop fs –mkdir input
# delete directory and all it's contents
hadoop fs –rm -r input_old
					</code></pre>
				</section>

				<section>
					<h1>MapReduce</h1>
					<h3>Programming Model</h3>
				</section>

				<section>
					<h1>Map</h1>
					<h3>&nbsp;</h3>

					<pre><code data-trim contenteditable>
(in_key, in_value) -> (inter_key, inter_value)
					</code></pre>
				</section>

				<section>
					<h1>Map</h1>
					<h3>Example</h3>
					<pre><code data-trim contenteditable>
(k, v) -> k.toUpper(), v.toUpper())
					</code></pre>
				</section>

				<section>
					<h1>Map</h1>
					<h3>Example: Upper Case Mapper</h3>
					<pre><code data-trim contenteditable>
(k, v) -> k.toUpper(), v.toUpper())

('foo', 'bar') -> ('FOO', 'BAR')
('foo', 'other') -> ('FOO', 'OTHER')
('baz', 'more data') -> ('BAZ', 'MORE DATA')
					</code></pre>
				</section>

				<section>
					<h1>Map</h1>
					<h3>Example</h3>
					<pre><code data-trim contenteditable>
(k, v) ->
    foreach char c in v:
        emit (k, c)
					</code></pre>
				</section>

				<section>
					<h1>Map</h1>
					<h3>Example: Explode Mapper</h3>
					<pre><code data-trim contenteditable>
(k, v) ->
    foreach char c in v:
        emit (k, c)

('foo', 'bar') ->   ('foo', 'b'), ('foo', 'a'),
                    ('foo', 'r')
('baz', 'other') -> ('baz', 'o'), ('baz', 't'),
                    ('baz', 'h'), ('baz', 'e'),
                    ('baz', 'r')
					</code></pre>
				</section>

				<section>
					<h1>Map</h1>
					<h3>Example</h3>
					<pre><code data-trim contenteditable>
(k, v) ->
   if (isPrime(v)) then emit(k, v)
					</code></pre>
				</section>

				<section>
					<h1>Map</h1>
					<h3>Example: Filter Mapper</h3>
					<pre><code data-trim contenteditable>
(k, v) ->
   if (isPrime(v)) then emit(k, v)

('foo', 7) -> ('foo', 7)
('baz', 10) -> nothing
					</code></pre>
				</section>

				<section>
					<h1>Map</h1>
					<h3>Example: Changing key</h3>
					<p>Output key must not be identical</p>
					<pre><code data-trim contenteditable>
(k, v) ->
    emit(v.length(), v)
					</code></pre>
				</section>

				<section>
					<h1>Map</h1>
					<h3>Example: Changing key</h3>
					<pre><code data-trim contenteditable>
(k, v) ->
    emit(v.length(), v)

('foo', 'bar') ->   (3, 'bar')
('baz', 'other') -> (5, 'other')
('foo', 'abracadabra') -> (11, 'abracadabra')
					</code></pre>
				</section>

				<section>
					<h1>Shuffle and Sort</h1>
					<h3>&nbsp;</h3>
					<ul>
						<li>all the values for a given key are combined to a list</li>
						<li>list is given to a reducer</li>
						<li>all values of particular key go to same reducer</li>
						<li>reducer inputs are merged and</li>
						<li>sorted by key</li>
				</section>

				<section>
					<h1>Shuffle and Sort</h1>
					<h3>Problems?</h3>
				</section>

				<section>
					<h1>Shuffle and Sort</h1>
					<h3>Problems</h3>
					<ul>
						<li>we have to wait for all mappers to finish before reducing can start</li>
						<li>in practice Hadoop starts before</li>
						<li>slow mappers are "handled" with speculative execution</li>
					</ul>
				</section>

				<section>
					<h1>Reduce</h1>
					<h3>&nbsp;</h3>

					<pre><code data-trim contenteditable>
(key, [vals]) -> (inter_key, inter_value)
					</code></pre>

					<ul>
						<li>reducer outputs zero or more final key/value pairs</li>
						<li>usually reducer emits single key/value pair for each input key</li>
					</ul>
				</section>

				<section>
					<h1>Reduce</h1>
					<h3>Example</h3>
					<pre><code data-trim contenteditable>
(k, [vals]) ->
    sum = 0
    foreach int i in [vals]:
        sum += i
    emit(k, sum)
					</code></pre>
				</section>

				<section>
					<h1>Reducer</h1>
					<h3>Example: Sum Reducer</h3>
					<pre><code data-trim contenteditable>
(k, [vals]) ->
    sum = 0
    foreach int i in [vals]:
        sum += i
    emit(k, sum)

(’bar', [9, 3, -17, 44]) -> (’bar', 39)
(’foo', [123, 100, 77]) -> (’foo', 300)
					</code></pre>
				</section>

			<section>
					<h1>MapReduce</h1>
					<h3>Example: Word Count</h3>
					<pre><code data-trim contenteditable>
# Input
# (linenumber, line)
(3414, 'the cat sat on the mat')
(3437, 'the aardvark sat on the sofa')

# Output
# (word, occurences)
('aardvark', 1)
('cat', 1)
('mat', 1)
('on', 2)
('sat', 2)
('sofa', 1)
('the', 4)
					</code></pre>
				</section>

				<section>
					<h1>MapReduce</h1>
					<h3>Example: Word Count</h3>
					<pre><code data-trim contenteditable>
map(String input_key, String input_value)
    foreach word w in input_value:
        emit(w, 1)

# Input
(3414, 'the cat sat on the mat')
(3437, 'the aardvark sat on the sofa')
					</code></pre>
				</section>

				<section>
					<h1>MapReduce</h1>
					<h3>Example: Word Count</h3>
					<pre><code data-trim contenteditable>
map(String input_key, String input_value)
    foreach word w in input_value:
        emit(w, 1)

# Output
('the', 1), ('cat', 1), ('sat', 1), ('on', 1),
('the', 1), ('mat', 1), ('the', 1), ('aardvark', 1),
('sat', 1), ('on', 1), ('the', 1), ('sofa', 1)
					</code></pre>
				</section>

				<section>
					<h1>MapReduce</h1>
					<h3>Example: Word Count</h3>

					<p>Shuffle and Sort</p>
					<pre><code data-trim contenteditable>
('aardvark', [1])
('cat', [1])
('mat', [1])
('on', [1, 1])
('sat', [1, 1])
('sofa', [1])
('the', [1, 1, 1, 1])
					</code></pre>
				</section>

				<section>
					<h1>MapReduce</h1>
					<h3>Example: Word Count</h3>
					<pre><code data-trim contenteditable>
reduce(String output_key, Iterator<Integer> intermediate_vals)
    set count = 0
    foreach v in intermediate_vals:
        count += v
    emit(output_key, count)

# Input
('aardvark', [1])
('cat', [1])
('mat', [1])
('on', [1, 1])
('sat', [1, 1])
('sofa', [1])
('the', [1, 1, 1, 1])
					</code></pre>
				</section>

				<section>
					<h1>MapReduce</h1>
					<h3>Example: Word Count</h3>
					<pre><code data-trim contenteditable>
reduce(String output_key, Iterator<Integer> intermediate_vals)
    set count = 0
    foreach v in intermediate_vals:
        count += v
    emit(output_key, count)

# Output
('aardvark', 1)
('cat', 1)
('mat', 1)
('on', 2)
('sat', 2)
('sofa', 1)
('the', 4)
					</code></pre>
				</section>

				<section>
					<h1>Hadoop</h1>
					<h3>Let's talk API</h3>

					<ul>
						<li>Driver</li>
						<li>Mapper</li>
						<li>Reducer</li>
					</ul>
				</section>

				<section>
					<h1>Hadoop</h1>
					<h3>Demo</h3>
				</section>

				<section>
					<h1>Hadoop</h1>
					<h3>Combiner</h3>

					<ul>
						<li>like a mini reducer</li>
						<li>runs locally and decreases network traffic</li>
						<li>IMPORTANT: may run multiple times</li>
					</ul>
				</section>

				<section>
					<h1>Hadoop</h1>
					<h3>Partitioner</h3>

					<ul>
						<li>partions keyspace</li>
						<li>decides which key goes to which reducer</li>
						<li>helpful for load balancing: e.g. log analysis</li>
					</ul>
				</section>

			</div>

		</div>

		<script src="lib/js/head.min.js"></script>
		<script src="js/reveal.min.js"></script>

		<script>

			// Full list of configuration options available here:
			// https://github.com/hakimel/reveal.js#configuration
			Reveal.initialize({
				controls: true,
				progress: true,
				history: true,
				center: true,

				theme: Reveal.getQueryHash().theme, // available themes are in /css/theme
				transition: Reveal.getQueryHash().transition || 'default', // default/cube/page/concave/zoom/linear/fade/none

				// Parallax scrolling
				// parallaxBackgroundImage: 'https://s3.amazonaws.com/hakim-static/reveal-js/reveal-parallax-1.jpg',
				// parallaxBackgroundSize: '2100px 900px',

				// Optional libraries used to extend on reveal.js
				dependencies: [
					{ src: 'lib/js/classList.js', condition: function() { return !document.body.classList; } },
					{ src: 'plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
					{ src: 'plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
					{ src: 'plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },
					{ src: 'plugin/zoom-js/zoom.js', async: true, condition: function() { return !!document.body.classList; } },
					{ src: 'plugin/notes/notes.js', async: true, condition: function() { return !!document.body.classList; } }
				]
			});

		</script>

	</body>
</html>
